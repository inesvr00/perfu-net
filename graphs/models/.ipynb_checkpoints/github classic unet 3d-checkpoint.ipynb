{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv3D(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, k, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.k = k\n",
    "        \n",
    "        self.inc = DoubleConv3D(n_channels, k)\n",
    "        self.down1 = Down(k, k*2**1)\n",
    "        self.down2 = Down(k*2**1, k*2**2)\n",
    "        self.down3 = Down(k*2**2, k*2**3)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(k*2**3, k*2**4 // factor)\n",
    "        self.up1 = Up(k*2**4, k*2**3 // factor, bilinear)\n",
    "        self.up2 = Up(k*2**3, k*2**2 // factor, bilinear)\n",
    "        self.up3 = Up(k*2**2, k*2**1 // factor, bilinear)\n",
    "        self.up4 = Up(k*2**1, k, bilinear)\n",
    "        self.outc = OutConv(k, n_classes)\n",
    "        \n",
    "#         self.inc = DoubleConv(n_channels, 64)\n",
    "#         self.down1 = Down(64, 128)\n",
    "#         self.down2 = Down(128, 256)\n",
    "#         self.down3 = Down(256, 512)\n",
    "#         factor = 2 if bilinear else 1\n",
    "#         self.down4 = Down(512, 1024 // factor)\n",
    "#         self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "#         self.up2 = Up(512, 256 // factor, bilinear)\n",
    "#         self.up3 = Up(256, 128 // factor, bilinear)\n",
    "#         self.up4 = Up(128, 64, bilinear)\n",
    "#         self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x5 = x5.squeeze()\n",
    "        x = self.up1(x5, torch.mean(x4, dim=2))\n",
    "        x = self.up2(x, torch.mean(x3, dim=2))\n",
    "        x = self.up3(x, torch.mean(x2, dim=2))\n",
    "        x = self.up4(x, torch.mean(x1, dim=2))\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1     [-1, 16, 16, 256, 256]             880\n",
      "       BatchNorm3d-2     [-1, 16, 16, 256, 256]              32\n",
      "              ReLU-3     [-1, 16, 16, 256, 256]               0\n",
      "            Conv3d-4     [-1, 16, 16, 256, 256]           6,928\n",
      "       BatchNorm3d-5     [-1, 16, 16, 256, 256]              32\n",
      "              ReLU-6     [-1, 16, 16, 256, 256]               0\n",
      "      DoubleConv3D-7     [-1, 16, 16, 256, 256]               0\n",
      "         MaxPool3d-8      [-1, 16, 8, 128, 128]               0\n",
      "            Conv3d-9      [-1, 32, 8, 128, 128]          13,856\n",
      "      BatchNorm3d-10      [-1, 32, 8, 128, 128]              64\n",
      "             ReLU-11      [-1, 32, 8, 128, 128]               0\n",
      "           Conv3d-12      [-1, 32, 8, 128, 128]          27,680\n",
      "      BatchNorm3d-13      [-1, 32, 8, 128, 128]              64\n",
      "             ReLU-14      [-1, 32, 8, 128, 128]               0\n",
      "     DoubleConv3D-15      [-1, 32, 8, 128, 128]               0\n",
      "             Down-16      [-1, 32, 8, 128, 128]               0\n",
      "        MaxPool3d-17        [-1, 32, 4, 64, 64]               0\n",
      "           Conv3d-18        [-1, 64, 4, 64, 64]          55,360\n",
      "      BatchNorm3d-19        [-1, 64, 4, 64, 64]             128\n",
      "             ReLU-20        [-1, 64, 4, 64, 64]               0\n",
      "           Conv3d-21        [-1, 64, 4, 64, 64]         110,656\n",
      "      BatchNorm3d-22        [-1, 64, 4, 64, 64]             128\n",
      "             ReLU-23        [-1, 64, 4, 64, 64]               0\n",
      "     DoubleConv3D-24        [-1, 64, 4, 64, 64]               0\n",
      "             Down-25        [-1, 64, 4, 64, 64]               0\n",
      "        MaxPool3d-26        [-1, 64, 2, 32, 32]               0\n",
      "           Conv3d-27       [-1, 128, 2, 32, 32]         221,312\n",
      "      BatchNorm3d-28       [-1, 128, 2, 32, 32]             256\n",
      "             ReLU-29       [-1, 128, 2, 32, 32]               0\n",
      "           Conv3d-30       [-1, 128, 2, 32, 32]         442,496\n",
      "      BatchNorm3d-31       [-1, 128, 2, 32, 32]             256\n",
      "             ReLU-32       [-1, 128, 2, 32, 32]               0\n",
      "     DoubleConv3D-33       [-1, 128, 2, 32, 32]               0\n",
      "             Down-34       [-1, 128, 2, 32, 32]               0\n",
      "        MaxPool3d-35       [-1, 128, 1, 16, 16]               0\n",
      "           Conv3d-36       [-1, 256, 1, 16, 16]         884,992\n",
      "      BatchNorm3d-37       [-1, 256, 1, 16, 16]             512\n",
      "             ReLU-38       [-1, 256, 1, 16, 16]               0\n",
      "           Conv3d-39       [-1, 256, 1, 16, 16]       1,769,728\n",
      "      BatchNorm3d-40       [-1, 256, 1, 16, 16]             512\n",
      "             ReLU-41       [-1, 256, 1, 16, 16]               0\n",
      "     DoubleConv3D-42       [-1, 256, 1, 16, 16]               0\n",
      "             Down-43       [-1, 256, 1, 16, 16]               0\n",
      "  ConvTranspose2d-44          [-1, 128, 32, 32]         131,200\n",
      "           Conv2d-45          [-1, 128, 32, 32]         295,040\n",
      "      BatchNorm2d-46          [-1, 128, 32, 32]             256\n",
      "             ReLU-47          [-1, 128, 32, 32]               0\n",
      "           Conv2d-48          [-1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-49          [-1, 128, 32, 32]             256\n",
      "             ReLU-50          [-1, 128, 32, 32]               0\n",
      "       DoubleConv-51          [-1, 128, 32, 32]               0\n",
      "               Up-52          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-53           [-1, 64, 64, 64]          32,832\n",
      "           Conv2d-54           [-1, 64, 64, 64]          73,792\n",
      "      BatchNorm2d-55           [-1, 64, 64, 64]             128\n",
      "             ReLU-56           [-1, 64, 64, 64]               0\n",
      "           Conv2d-57           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-58           [-1, 64, 64, 64]             128\n",
      "             ReLU-59           [-1, 64, 64, 64]               0\n",
      "       DoubleConv-60           [-1, 64, 64, 64]               0\n",
      "               Up-61           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-62         [-1, 32, 128, 128]           8,224\n",
      "           Conv2d-63         [-1, 32, 128, 128]          18,464\n",
      "      BatchNorm2d-64         [-1, 32, 128, 128]              64\n",
      "             ReLU-65         [-1, 32, 128, 128]               0\n",
      "           Conv2d-66         [-1, 32, 128, 128]           9,248\n",
      "      BatchNorm2d-67         [-1, 32, 128, 128]              64\n",
      "             ReLU-68         [-1, 32, 128, 128]               0\n",
      "       DoubleConv-69         [-1, 32, 128, 128]               0\n",
      "               Up-70         [-1, 32, 128, 128]               0\n",
      "  ConvTranspose2d-71         [-1, 16, 256, 256]           2,064\n",
      "           Conv2d-72         [-1, 16, 256, 256]           4,624\n",
      "      BatchNorm2d-73         [-1, 16, 256, 256]              32\n",
      "             ReLU-74         [-1, 16, 256, 256]               0\n",
      "           Conv2d-75         [-1, 16, 256, 256]           2,320\n",
      "      BatchNorm2d-76         [-1, 16, 256, 256]              32\n",
      "             ReLU-77         [-1, 16, 256, 256]               0\n",
      "       DoubleConv-78         [-1, 16, 256, 256]               0\n",
      "               Up-79         [-1, 16, 256, 256]               0\n",
      "           Conv2d-80          [-1, 1, 256, 256]              17\n",
      "          OutConv-81          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 4,299,169\n",
      "Trainable params: 4,299,169\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 1393.25\n",
      "Params size (MB): 16.40\n",
      "Estimated Total Size (MB): 1417.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(UNet(2,1,16, True), input_size=(2,16, 256, 256), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
